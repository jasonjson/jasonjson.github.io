---
layout: post
title: Deep Learning Notes
date: 2017-01-07 13:31
categories:
- A note
author: Jason
---
<p><strong><em>Deep learning notes</em></strong></p>

# Implementing neural network

## The delta Rule and learning Rates
* Convergence problem: if we pick a learning rate that’s too small, we risk taking too long during the training process. But if we pick a learning rate that’s too big, we’ll mostly likely start diverging away from the minimum.

## Gradient Descent with Sigmoidal Neurons
* The new modification rule is just like the delta rule, except with extra multiplicative terms included to account for the logistic component of the sigmoidal neuron.

## The back-propagation Algorithm



